{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy.core._multiarray_umath'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'numpy.core._multiarray_umath'"
     ]
    },
    {
     "ename": "SystemError",
     "evalue": "<class '_frozen_importlib._ModuleLockManager'> returned a result with an error set",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;31mImportError\u001b[0m: numpy.core.multiarray failed to import",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mSystemError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "\u001b[1;31mSystemError\u001b[0m: <class '_frozen_importlib._ModuleLockManager'> returned a result with an error set"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "numpy.core._multiarray_umath failed to import",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;31mImportError\u001b[0m: numpy.core._multiarray_umath failed to import"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "numpy.core.umath failed to import",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;31mImportError\u001b[0m: numpy.core.umath failed to import"
     ]
    }
   ],
   "source": [
    "#===========================================================================================\n",
    "#===========================================================================================\n",
    "#=================================== LSTM FOR FORECASTING ==================================\n",
    "#===========================================================================================\n",
    "#===========================================================================================\n",
    "\n",
    "\n",
    "from pandas import DataFrame\n",
    "import pandas as pd\n",
    "\n",
    "from pandas import Series\n",
    "from pandas import concat\n",
    "from pandas import read_csv\n",
    "from pandas import datetime\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "\n",
    "from math import sqrt\n",
    "\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from matplotlib import pyplot\n",
    "from matplotlib.pylab import rcParams\n",
    "\n",
    "pyplot.style.use('fivethirtyeight')\n",
    "rcParams['figure.figsize'] = 15, 6\n",
    "\n",
    "from numpy import array\n",
    "import numpy as np\n",
    "\n",
    "import winsound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# date-time parsing function for loading the dataset\n",
    "def parser(x):\n",
    "\treturn datetime.strptime('190'+x, '%Y-%m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert time series into supervised learning problem\n",
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "\tn_vars = 1 if type(data) is list else data.shape[1]\n",
    "\tdf = DataFrame(data)\n",
    "\tcols, names = list(), list()\n",
    "\t# input sequence (t-n, ... t-1)\n",
    "\tfor i in range(n_in, 0, -1):\n",
    "\t\tcols.append(df.shift(i))\n",
    "\t\tnames += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "\t# forecast sequence (t, t+1, ... t+n)\n",
    "\tfor i in range(0, n_out):\n",
    "\t\tcols.append(df.shift(-i))\n",
    "\t\tif i == 0:\n",
    "\t\t\tnames += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "\t\telse:\n",
    "\t\t\tnames += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "\t# put it all together\n",
    "\tagg = concat(cols, axis=1)\n",
    "\tagg.columns = names\n",
    "\t# drop rows with NaN values\n",
    "\tif dropnan:\n",
    "\t\tagg.dropna(inplace=True)\n",
    "        \n",
    "\treturn agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a differenced series\n",
    "def difference(dataset, interval=1):\n",
    "\tdiff = list()\n",
    "\tfor i in range(interval, len(dataset)):\n",
    "\t\tvalue = dataset[i] - dataset[i - interval]\n",
    "\t\tdiff.append(value)\n",
    "        \n",
    "\treturn Series(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overwrite NaNs with column value interpolations.\n",
    "def interpolate_nans(X):\n",
    "    for j in range(X.shape[1]):\n",
    "        mask_j = np.isnan(X[:,j])\n",
    "        X[mask_j,j] = np.interp(np.flatnonzero(mask_j), np.flatnonzero(~mask_j), X[~mask_j,j])\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform series into train and test sets for supervised learning\n",
    "def prepare_data(series, n_test, n_lag, n_seq):\n",
    "    \n",
    "\t# extract raw values\n",
    "\traw_values = series\n",
    "\traw_values[np.isnan(raw_values)] = 0\n",
    "    \n",
    "\t# transform data to be stationary\n",
    "\tdiff_series = difference(raw_values, 1)\n",
    "\tdiff_values = diff_series.values\n",
    "\tdiff_values = diff_values.reshape(len(diff_values), 1)\n",
    "    \n",
    "\t# rescale values to -1, 1\n",
    "\tscaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "\tscaled_values = scaler.fit_transform(diff_values) \n",
    "\tscaled_values = scaled_values.reshape(len(scaled_values), 1)\n",
    "    \n",
    "\t# transform into supervised learning problem X, y\n",
    "\tsupervised = series_to_supervised(scaled_values, n_lag, n_seq)\n",
    "\tsupervised_values = supervised.values\n",
    "    \n",
    "\t# split into train and test sets\n",
    "\ttrain, test = supervised_values[0:-n_test], supervised_values[-n_test:]\n",
    "    \n",
    "\treturn scaler, train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit an LSTM network to training data\n",
    "def fit_lstm(train, n_lag, n_seq, n_batch, nb_epoch, n_neurons):\n",
    "    \n",
    "\t# reshape training into [samples, timesteps, features]\n",
    "\tX, y = train[:, 0:n_lag], train[:, n_lag:]\n",
    "\tX = X.reshape(X.shape[0], 1, X.shape[1])\n",
    "    \n",
    "\t# design network\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(LSTM(n_neurons, batch_input_shape=(n_batch, X.shape[1], X.shape[2]), stateful=True))\n",
    "\tmodel.add(Dense(y.shape[1]))\n",
    "\tmodel.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    \n",
    "\t# fit network\n",
    "\tfor i in range(nb_epoch):\n",
    "\t\tmodel.fit(X, y, epochs=1, batch_size=n_batch, verbose=0, shuffle=False)\n",
    "\t\tmodel.reset_states()\n",
    "        \n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make one forecast with an LSTM,\n",
    "def forecast_lstm(model, X, n_batch):\n",
    "    \n",
    "\t# reshape input pattern to [samples, timesteps, features]\n",
    "\tX = X.reshape(1, 1, len(X))\n",
    "    \n",
    "\t# make forecast\n",
    "\tforecast = model.predict(X, batch_size=n_batch)\n",
    "    \n",
    "\t# convert to array    \n",
    "\treturn [x for x in forecast[0, :]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# invert differenced forecast\n",
    "def inverse_difference(last_ob, forecast):\n",
    "    \n",
    "\t# invert first forecast\n",
    "\tinverted = list()\n",
    "\tinverted.append(forecast[0] + last_ob)\n",
    "    \n",
    "\t# propagate difference forecast using inverted first value\n",
    "\tfor i in range(1, len(forecast)):\n",
    "\t\tinverted.append(forecast[i] + inverted[i-1])\n",
    "        \n",
    "\treturn inverted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the persistence model\n",
    "def make_forecasts(model, n_batch, train, test, n_lag, n_seq):\n",
    "\tforecasts = list()\n",
    "\tfor i in range(len(test)):\n",
    "\t\tX, y = test[i, 0:n_lag], test[i, n_lag:]\n",
    "        \n",
    "\t\t# make forecast\n",
    "\t\tforecast = forecast_lstm(model, X, n_batch)\n",
    "        \n",
    "\t\t# store the forecast\n",
    "\t\tforecasts.append(forecast)\n",
    "        \n",
    "\treturn forecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inverse data transform on forecasts\n",
    "def inverse_transform(series, forecasts, scaler, n_test):\n",
    "    \n",
    "\tinverted = list()\n",
    "\tfor i in range(len(forecasts)):\n",
    "        \n",
    "\t\t# create array from forecast\n",
    "\t\tforecast = array(forecasts[i])\n",
    "\t\tforecast = forecast.reshape(1, len(forecast))\n",
    "        \n",
    "\t\t# invert scaling\n",
    "\t\tinv_scale = scaler.inverse_transform(forecast)\n",
    "\t\tinv_scale = inv_scale[0, :]\n",
    "        \n",
    "\t\t# invert differencing\n",
    "\t\tindex = len(series) - n_test + i - 1\n",
    "\t\tlast_ob = series.values[index]\n",
    "\t\tinv_diff = inverse_difference(last_ob, inv_scale)\n",
    "        \n",
    "\t\t# store\n",
    "\t\tinverted.append(inv_diff)\n",
    "        \n",
    "\treturn inverted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the RMSE for each forecast time step\n",
    "\n",
    "def evaluate_forecasts(test, forecasts, n_lag, n_seq, filename):\n",
    "\tdfxx = pd.DataFrame(columns=['FILENAME','PREDICTION #','MEAN ABSOLUTE ERROR','MEAN SQUARED ERROR','ROOT MEAN SQUARED ERROR'], index = [1])\n",
    "    \n",
    "\tfor i in range(n_seq):\n",
    "\t\tactual = [row[i] for row in test]\n",
    "\t\tpredicted = [forecast[i] for forecast in forecasts]\n",
    "          \n",
    "\t\tMAE = mean_absolute_error(actual, predicted)       \n",
    "\t\tMSE = mean_squared_error(actual, predicted)\n",
    "\t\tRMSE = sqrt(mean_squared_error(actual, predicted))\n",
    "    \n",
    "\t\tdfxx = dfxx.append({'FILENAME': filename, 'PREDICTION #': (i+1), 'MEAN ABSOLUTE ERROR': MAE, \n",
    "                     'MEAN SQUARED ERROR': MSE, 'ROOT MEAN SQUARED ERROR': RMSE}, ignore_index=True)      \n",
    "        \n",
    "# original code printed RMSE; replaced by storing to array and transferring to main()        \n",
    "#\t\tprint('t+%d RMSE: %f' % ((i+1), rmse))\n",
    "\n",
    "#\tprint('df: ', dfxx)\n",
    "\n",
    "\treturn dfxx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "\n",
    "# plot the forecasts in the context of the original dataset\n",
    "def plot_forecasts(series, Dates, filename, forecasts, n_test, showplot):\n",
    "    \n",
    "\trcParams['figure.figsize'] = 15, 6\n",
    "    \n",
    "\t# plot the entire dataset in blue\n",
    "\tpyplot.plot(series.values, color='blue')\n",
    "        \n",
    "\t# picks a color theme for the prediction lines\n",
    "\tcmap = mpl.cm.cool\n",
    "    \n",
    "\t# plot the forecasts in red\n",
    "\tfor i in range(len(forecasts)):        \n",
    "        \n",
    "\t\toff_s = len(series) - n_test + i - 1\n",
    "\t\toff_e = off_s + len(forecasts[i]) + 1\n",
    "\t\txaxis = [x for x in range(off_s, off_e)]\n",
    "\t\tyaxis = [series.values[off_s]] + forecasts[i]\n",
    "        \n",
    "\t\tpyplot.title(filename, loc='center') \n",
    "\t\tpyplot.plot(xaxis, yaxis, ':', color=cmap(i / float(len(forecasts))), lw=3)\n",
    "        \n",
    "\t\tif i == 0:            \n",
    "\t\t\td =  {('Prediction'+str(i+1)): pd.Series([series.values[off_s]] + forecasts[i], index=[xaxis])}    \n",
    "\t\t\tdf = pd.DataFrame(d) \n",
    "\n",
    "\t\telse:\n",
    "\t\t\td2 = {('Prediction'+str(i+1)): pd.Series([series.values[off_s]] + forecasts[i], index=[xaxis])} \n",
    "\t\t\tdf2 = pd.DataFrame(d2)\n",
    "\t\t\tdf=df.add(df2, fill_value=0)\n",
    "\n",
    "\t# show the plot\n",
    "\tif showplot == 'y':\n",
    "\t\tpyplot.savefig(filename.replace(\".csv\", \"\") + \"_PREDICTIONS.jpg\")\n",
    "\t\tpyplot.show()\n",
    "\n",
    "\telse:\n",
    "\t\tpyplot.close()\n",
    "        \n",
    "\tdel d\n",
    "\tdel d2\n",
    "\tdel df2\n",
    "\treturn df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yes_or_no(question):\n",
    "\tanswer = input(question + \"(Y/N): \").upper().strip()\n",
    "\n",
    "\twhile not(answer == \"Y\" or answer == \"N\"):\n",
    "\t\tprint(\"Input Y or N\")\n",
    "\t\tanswer = input(question + \"(Y/N): \").upper().strip()\n",
    "\tif answer[0] == \"Y\":\n",
    "\t\treturn True\n",
    "\telse:\n",
    "\t\treturn False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def additive_seasonal_decomp(series, frq):\n",
    "\trcParams['figure.figsize'] = 15, 7\n",
    "\n",
    "\tresult = seasonal_decompose(series.values, model='additive', freq=1)\n",
    "\tresult.plot()\n",
    "\tprint('Additive Decomposition Plot')\n",
    "\tpyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiplicative_seasonal_decomp(series, frq):\n",
    "\trcParams['figure.figsize'] = 15, 7\n",
    " \n",
    "\tresult = seasonal_decompose(series.values, model='multiplicative', freq=1)\n",
    "\tresult.plot()\n",
    "\tprint('Multiplicative Decomposition Plot')\n",
    "\tpyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = []\n",
    "#filenames = ['ge.us.csv']\n",
    "#filenames = ['WKHS.csv']\n",
    "filenames = ['FIBER_NODE_DI_PREDICTABILITY.CSV']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Time:  16:44:39\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-c5e520f531e1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Start Time: '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtstarta\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrftime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"%H:%M:%S\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mRMSE_ARR\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0mdf3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "#===========================================================================================\n",
    "#========================================== PREP ===========================================\n",
    "#===========================================================================================\n",
    "# loop through all datasets chosen in numpy array above\n",
    "\n",
    "# set ending beep duration in seconds\n",
    "beepduration = 1\n",
    "\n",
    "tstarta = datetime.now()\n",
    "print('Start Time: ', tstarta.strftime(\"%H:%M:%S\"))\n",
    "\n",
    "RMSE_ARR = np.array([])\n",
    "df = []\n",
    "df3 = []\n",
    "\n",
    "for index, item in enumerate(filenames):\n",
    "          \n",
    "        # set start time for execution time measurement\n",
    "        tstart = datetime.now()\n",
    "\n",
    "        # load dataset\n",
    "        Actuals = read_csv(item)           \n",
    "        Actuals.Timestamp = pd.to_datetime(Actuals.DT,format='%m/%d/%Y')     \n",
    "        Actuals.index = Actuals.Timestamp  \n",
    "        Actuals.drop('DT', axis=1, inplace=True) \n",
    "        \n",
    "        #Actuals = Actuals.asfreq('M')\n",
    "        #Actuals = Actuals.resample('M').reset_index()\n",
    "        Actuals = Actuals.bfill()\n",
    "        #print(Actuals)\n",
    "\n",
    "        #Actuals.set_index('DT', inplace=True)\n",
    "        #Actuals = Actuals.asfreq('M')\n",
    "        #print(Actuals)\n",
    "\n",
    "        Dates = read_csv(item, usecols=['DT']) \n",
    "\n",
    "        # configure\n",
    "        startpoint = 1  # how far back to start predictions\n",
    "        n_lag = 1        # amount of lag included in prediction points\n",
    "        n_seq = 13       # number of prediction points per prediction iteration\n",
    "        n_test = 6       # number of visualized predictions iterations\n",
    "        n_epochs = 100   # number of iterations to improve predictive outcome\n",
    "        n_batch = 1      # must be 1\n",
    "        n_neurons = 400   # number of confounding nodes in prediction calculation\n",
    "\n",
    "        # prepare data\n",
    "        scaler, train, test = prepare_data(Actuals.TOTAL_ROWS, n_test, n_lag, n_seq)\n",
    "    \n",
    "#===========================================================================================\n",
    "#========================================== RUN ============================================\n",
    "#===========================================================================================\n",
    "\n",
    "        # plot seasonal decomposition\n",
    "            # Level: The average value in the series.\n",
    "            # Trend: The increasing or decreasing value in the series.\n",
    "            # Seasonality: The repeating short-term cycle in the series.\n",
    "            # Noise: The random variation in the series.   \n",
    "        #additive_seasonal_decomp(Actuals.LIVES, 7)\n",
    "        #multiplicative_seasonal_decomp(Actuals.LIVES, 7)            \n",
    "\n",
    "        # plot autocorrelation\n",
    "        rcParams['figure.figsize'] = 15, 5\n",
    "        #plot_acf(Actuals, lags=500)\n",
    "        plot_acf(Actuals.TOTAL_ROWS)\n",
    "        pyplot.savefig(item.replace(\".csv\", \"\") + \"_AUTOCORRELATION.jpg\", bbox_inches='tight')\n",
    "        pyplot.show()      \n",
    "\n",
    "        # fit model\n",
    "        model = fit_lstm(train, n_lag, n_seq, n_batch, n_epochs, n_neurons)\n",
    "\n",
    "        # make forecasts\n",
    "        forecasts = make_forecasts(model, n_batch, train, test, n_lag, n_seq)\n",
    "\n",
    "        # inverse transform forecasts and test\n",
    "        forecasts = inverse_transform(Actuals.TOTAL_ROWS, forecasts, scaler, n_test + startpoint)\n",
    "        actual = [row[n_lag:] for row in test]\n",
    "        actual = inverse_transform(Actuals.TOTAL_ROWS, actual, scaler, n_test + startpoint)\n",
    "\n",
    "        # evaluate forecasts\n",
    "        #evaluate_forecasts(actual, forecasts, n_lag, n_seq, item + '_Zero_nan')    \n",
    "        RMSE_ARR_df = evaluate_forecasts(actual, forecasts, n_lag, n_seq, item + '_Zero_nan')          \n",
    "\n",
    "        # plot forecasts\n",
    "        df = Actuals\n",
    "        \n",
    "        \n",
    "        df3 = (plot_forecasts(Actuals.TOTAL_ROWS, Dates, item, forecasts, n_test + startpoint, 'y'))              \n",
    "        df3['DT'] = ['2018-12-01','2019-01-01','2019-02-01','2019-03-01','2019-04-01','2019-05-01','2019-06-01','2019-07-01','2019-08-01','2019-09-01','2019-10-01','2019-11-01','2019-12-01','2019-12-02','2019-12-03','2019-12-04','2019-12-05','2019-12-06','2019-12-07']\n",
    "        df3.set_index('DT', inplace=True)           \n",
    "        results = pd.concat([df, df3])\n",
    "        \n",
    "#        print(df)\n",
    "#        print(RMSE_ARR_df)\n",
    "                           \n",
    "        del Actuals\n",
    "        del Dates\n",
    "\n",
    "        tend = datetime.now()\n",
    "        totaltime = tend - tstart\n",
    "        d = datetime(1,1,1) + totaltime\n",
    "\n",
    "        results.to_csv(item.replace(\".csv\", \"\") + \"_VALUES.CSV\")\n",
    "        RMSE_ARR_df.to_csv(item.replace(\".csv\", \"\") + \"_RMSE.CSV\")     \n",
    "            \n",
    "#===========================================================================================\n",
    "#========================================== DONE ===========================================\n",
    "#===========================================================================================\n",
    "\n",
    "frequency = 750  # Set Frequency To 2500 Hertz\n",
    "duration = beepduration*1000  # Set Duration To 1000 ms == 1 second\n",
    "winsound.Beep(frequency, duration)\n",
    "\n",
    "tendd = datetime.now()\n",
    "totaltimex = tendd - tstarta\n",
    "d = datetime(1,1,1) + totaltimex\n",
    "print('Total Time Elapsed (hh:mm:ss.ms) {}'.format(totaltime))\n",
    "\n",
    "if yes_or_no(\"Print RMSE for models with nulls replaced with 0?: \") == True:\n",
    "    print(RMSE_ARR)\n",
    "\n",
    "del RMSE_ARR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# In[41]:\n",
    "\n",
    "#===========================================================================================\n",
    "#===========================================================================================\n",
    "#==================================== LSTM TIMER TEST ======================================\n",
    "#===========================================================================================\n",
    "#===========================================================================================\n",
    "\n",
    "\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "# use to insure memory is cleared after run.\n",
    "# import sys\n",
    "# def sizeof_fmt(num, suffix='B'):\n",
    "#     ''' By Fred Cirera, after https://stackoverflow.com/a/1094933/1870254'''\n",
    "#     for unit in ['','Ki','Mi','Gi','Ti','Pi','Ei','Zi']:\n",
    "#         if abs(num) < 1024.0:\n",
    "#             return \"%3.1f%s%s\" % (num, unit, suffix)\n",
    "#         num /= 1024.0\n",
    "#     return \"%.1f%s%s\" % (num, 'Yi', suffix)\n",
    "\n",
    "# for name, size in sorted(((name, sys.getsizeof(value)) for name,value in locals().items()),\n",
    "#                          key= lambda x: -x[1])[:10]:\n",
    "#     print(\"{:>30}: {:>8}\".format(name,sizeof_fmt(size)))\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[154]:\n",
    "\n",
    "from pandas import DataFrame\n",
    "import pandas as pd\n",
    "\n",
    "from pandas import Series\n",
    "from pandas import concat\n",
    "from pandas import read_csv\n",
    "from pandas import datetime\n",
    "from matplotlib import pyplot\n",
    "\n",
    "from numpy import array\n",
    "import numpy as np\n",
    "\n",
    "import winsound\n",
    "\n",
    "\n",
    "# In[155]:\n",
    "\n",
    "from scipy.stats import boxcox\n",
    "from scipy.special import inv_boxcox\n",
    "\n",
    "\n",
    "# In[156]:\n",
    "\n",
    "import fbprophet\n",
    "\n",
    "\n",
    "# In[157]:\n",
    "\n",
    "# load dataset\n",
    "Actuals = read_csv('AMAZON2.csv')       \n",
    "Actuals.Timestamp = pd.to_datetime(Actuals.DT,format='%m/%d/%Y')  \n",
    "Actuals.index = Actuals.Timestamp  \n",
    "Actuals.drop('DT', axis=1, inplace=True) \n",
    "\n",
    "Actuals = Actuals.asfreq('d')\n",
    "Actuals = Actuals.resample('d').bfill()\n",
    "Actuals = Actuals.bfill()\n",
    "\n",
    "\n",
    "# In[158]:\n",
    "\n",
    "df.shape\n",
    "\n",
    "\n",
    "# In[159]:\n",
    "\n",
    "df.dtypes\n",
    "\n",
    "\n",
    "# In[160]:\n",
    "\n",
    "print(Actuals.head(20))\n",
    "\n",
    "\n",
    "# In[162]:\n",
    "\n",
    "pyplot.plot(Actuals.index, Actuals.LIVES, ':', lw=3)\n",
    "pyplot.show()\n",
    "\n",
    "\n",
    "# In[163]:\n",
    "\n",
    "df['ds'] = df.index\n",
    "df['y'] = df['LIVES']\n",
    "\n",
    "\n",
    "# In[164]:\n",
    "\n",
    "# Apply Box-Cox Transform to value column and assign to new column y\n",
    "df['y'], lam = boxcox(df['LIVES'])\n",
    "\n",
    "\n",
    "# In[165]:\n",
    "\n",
    "m = fbprophet.Prophet()\n",
    "\n",
    "\n",
    "# In[169]:\n",
    "\n",
    "m.fit(df)\n",
    "\n",
    "\n",
    "# In[168]:\n",
    "\n",
    "future = m.make_future_dataframe(periods=12)\n",
    "\n",
    "print(future)\n",
    "# In[99]:\n",
    "\n",
    "forecast = m.predict(future)\n",
    "\n",
    "\n",
    "# In[100]:\n",
    "\n",
    "m.plot(forecast);\n",
    "\n",
    "\n",
    "# In[101]:\n",
    "\n",
    "m.plot_components(forecast);\n",
    "\n",
    "\n",
    "# In[102]:\n",
    "\n",
    "# Apply inverse Box-Cox transform to specific forecast columns\n",
    "forecast[['yhat','yhat_upper','yhat_lower']] = forecast[['yhat','yhat_upper','yhat_lower']].apply(lambda x: inv_boxcox(x, lam))\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "#import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#to plot within notebook\n",
    "import matplotlib.pyplot as plt\n",
    "get_ipython().magic('matplotlib inline')\n",
    "\n",
    "#setting figure size\n",
    "from matplotlib.pylab import rcParams\n",
    "rcParams['figure.figsize'] = 20,10\n",
    "\n",
    "#for normalizing data\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "#read the file\n",
    "df = pd.read_csv('ge.us.csv')\n",
    "\n",
    "#print the head\n",
    "#df.head()\n",
    "\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "#importing required libraries\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, LSTM\n",
    "\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "#creating dataframe\n",
    "data = df.sort_index(ascending=True, axis=0)\n",
    "new_data = pd.DataFrame(index=range(0,len(df)),columns=['DT', 'LIVES'])\n",
    "for i in range(0,len(data)):\n",
    "    new_data['DT'][i] = data['DT'][i]\n",
    "    new_data['LIVES'][i] = data['LIVES'][i]\n",
    "\n",
    "\n",
    "# In[4]:\n",
    "\n",
    "#setting index\n",
    "new_data.index = new_data.DT\n",
    "new_data.drop('DT', axis=1, inplace=True)\n",
    "\n",
    "#creating train and test sets\n",
    "dataset = new_data.values\n",
    "\n",
    "train = dataset[0:int(len(df)*.8),:]\n",
    "valid = dataset[int(len(df)*.8):,:]\n",
    "\n",
    "\n",
    "# In[5]:\n",
    "\n",
    "#converting dataset into x_train and y_train\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_data = scaler.fit_transform(dataset)\n",
    "\n",
    "x_train, y_train = [], []\n",
    "for i in range(60,len(train)):\n",
    "    x_train.append(scaled_data[i-60:i,0])\n",
    "    y_train.append(scaled_data[i,0])\n",
    "x_train, y_train = np.array(x_train), np.array(y_train)\n",
    "\n",
    "x_train = np.reshape(x_train, (x_train.shape[0],x_train.shape[1],1))\n",
    "\n",
    "\n",
    "# In[6]:\n",
    "\n",
    "# create and fit the LSTM network\n",
    "model = Sequential()\n",
    "model.add(LSTM(units=50, return_sequences=True, input_shape=(x_train.shape[1],1)))\n",
    "model.add(LSTM(units=50))\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "model.fit(x_train, y_train, epochs=1, batch_size=1, verbose=2)\n",
    "\n",
    "\n",
    "# In[7]:\n",
    "\n",
    "#predicting 20% of values, using past 60 from the train data\n",
    "inputs = new_data[len(new_data) - len(valid) - 60:].values\n",
    "inputs = inputs.reshape(-1,1)\n",
    "inputs  = scaler.transform(inputs)\n",
    "\n",
    "X_test = []\n",
    "for i in range(60,inputs.shape[0]):\n",
    "    X_test.append(inputs[i-60:i,0])\n",
    "X_test = np.array(X_test)\n",
    "\n",
    "X_test = np.reshape(X_test, (X_test.shape[0],X_test.shape[1],1))\n",
    "closing_price = model.predict(X_test)\n",
    "closing_price = scaler.inverse_transform(closing_price)\n",
    "\n",
    "\n",
    "# In[8]:\n",
    "\n",
    "rms=np.sqrt(np.mean(np.power((valid-closing_price),2)))\n",
    "print(rms)\n",
    "\n",
    "\n",
    "# In[13]:\n",
    "\n",
    "#for plotting\n",
    "train = new_data[:int(len(df)*.8)]\n",
    "valid = new_data[int(len(df)*.8):]\n",
    "\n",
    "valid['Predictions'] = closing_price\n",
    "\n",
    "plt.plot(train['LIVES'])\n",
    "plt.plot(valid[['LIVES','Predictions']])\n",
    "\n",
    "\n",
    "# In[12]:\n",
    "\n",
    "print('DONE')\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
